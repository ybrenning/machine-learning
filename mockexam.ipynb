{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probeklausur \"Grundlagen des Maschinellen Lernens\"\n",
    "\n",
    "## Aufgabe 1: Grundlagen und Evaluation\n",
    "\n",
    "(a) \n",
    "\n",
    "numerische Messwerte zu Gewicht, Größe und Körpertemperatur - Feature Space $ X $\n",
    "\n",
    "Menge der zu unterscheidenden Tierarten - Classes $ C $\n",
    "\n",
    "fachkundige:r Zoolog:in, welche:r Tiere identifiziert - $ \\gamma(o) $\n",
    "\n",
    "Waage, Thermometer, Messstab - $ \\alpha(o) $\n",
    "\n",
    "(b1)\n",
    "\n",
    "3\n",
    "\n",
    "(b2)\n",
    "\n",
    "Für die Klassifikatoren $ y_i'(), i = 1, \\dots, 3 $ haben die Trainings- und Testmengen jeweils die folgenden Anzahlen an Beispielen:\n",
    "\n",
    "$ |D_{test}| = 2 $\n",
    "\n",
    "$ |D_{tr}| = |D| - |D_{test}| = 6 - 2 = 4 $\n",
    "\n",
    "(b3)\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\text{Err}^*(y(), D, k) & = \\frac{1}{k} \\sum\\limits_{i = 1}^{k} \\frac{|(\\mathbf{x}, c) \\in D_{test_i}: y_i'(\\mathbf{x}) \\neq c|}{|D_{test_i}|} \\\\\n",
    "& = \\frac{1}{3} \\cdot (\\frac{1}{2} + \\frac{1}{2} + \\frac{0}{2}) \\\\\n",
    "& = \\frac{1}{3}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "## Aufgabe 2: Concept Learning\n",
    "\n",
    "Schritt 1:\n",
    "\n",
    "Inkonsistente Hypothesen aus $ H_S $ entfernen:\n",
    "\n",
    "$ H_S = \\{(sunny, warm, normal, ?, warm, same)\\} $\n",
    "\n",
    "Schritt 2:\n",
    "\n",
    "Entfernen von inkonsistenten Hypothesen aus $ H_G $:\n",
    "\n",
    "$ H_G = \\{(sunny, warm, ?, ?, ?, same)\\} $\n",
    "\n",
    "Hinzufügen von minimalen Spezialisierungen von $ \\{(sunny, ?, normal, ?, ?, same)\\} $:\n",
    "\n",
    "$ \\{(sunny, warm, normal, ?, ?, same), (sunny, cold, normal, ?, ?, same), (sunny, ?, normal, strong, ?, same), $\n",
    "\n",
    "$ (sunny, ?, normal, weak, ?, same), (sunny, ?, normal, ?, warm, same), (sunny, ?, normal, ?, cold, same)\\} $\n",
    "\n",
    "Consistent with $ \\mathbf{x}_1 $:\n",
    "\n",
    "$ \\{(sunny, warm, normal, ?, ?, same), (sunny, ?, normal, strong, ?, same), (sunny, ?, normal, ?, warm, same)\\} $\n",
    "\n",
    "Those with a more specific counterpart in $ H_S $:\n",
    "\n",
    "$ H_G = \\{(sunny, warm, ?, ?, ?, same), (sunny, warm, normal, ?, ?, same), (sunny, ?, normal, ?, warm, same)\\} $\n",
    "\n",
    "Remove hypothesis from $ H_G $ with less general counterparts in $ H_G $:\n",
    "\n",
    "$ H_G = \\{(sunny, warm, ?, ?, ?, same), (sunny, ?, normal, ?, warm, same)\\} $\n",
    "\n",
    "## Aufgabe 3: Lineare Modelle\n",
    "\n",
    "(a1)\n",
    "\n",
    "$ \\displaystyle y(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $\n",
    "\n",
    "(a2)\n",
    "\n",
    "In Zeile 7 wird die Ableitung der Loss-Funktion berechnet in Bezug zu $ \\mathbf{w} $ und mit der Lernrate multipliziert. Die skalierte Differenz wird anschließend in Zeile 8 verwendet, um $ \\mathbf{w} $ zu aktualisieren.\n",
    "\n",
    "(a3)\n",
    "\n",
    "In Zeile 6 und 7 wird die Ableitung der jeweiligen Loss-Funktion berechnet. Im Fall der linearen Regression ist es der squared loss $ l_2 $, im Fall der logistischen Regression wird der logistische Loss $ l_{\\sigma} $ verwendet.\n",
    "\n",
    "(b1)\n",
    "\n",
    "$ \\hat{y}(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^T \\mathbf{x}) $\n",
    "\n",
    "(b2)\n",
    "\n",
    "Goodness of fit as RSS:\n",
    "\n",
    "$ \\displaystyle \\sum\\limits_{(\\mathbf{x}, c) \\in D} (c - y(\\mathbf{x}))^2 $\n",
    "\n",
    "### Aufgabe 4: Bayesian Learning\n",
    "\n",
    "(a)\n",
    "\n",
    "Das Bayes-Theorem stellt den Zusammenhang zwischen Prior-, Posterior und Likelihood-Wahrscheinlichkeiten.\n",
    "\n",
    "$ \\displaystyle P(A_i \\mid B) = \\frac{P(A_i) \\cdot P(B \\mid A_i)}{\\sum\\limits_{i = 1}^{k} P(A_i) \\cdot P(B \\mid A_i)} $\n",
    "\n",
    "(b)\n",
    "\n",
    "Für den Naiven Bayes-Klassifikator $ A_{NB} = \\text{argmax}_{A_i \\in \\{A_1, \\dots, A_k\\}} P(A_i) \\cdot \\prod\\limits_{j = 1}^{p} P(B_j \\mid A_i) $ werden $ P(A_i) und P(B_j \\mid A_i) $ vorausgesetzt. In diesem Fall müsste also $ P(A_i), A_i := C = c_i $ und $ P(B_j \\mid A_i), A_i := C = c_i, B_j := X_j = x_j $  geschätzt werden.\n",
    "\n",
    "(c)\n",
    "\n",
    "Prior-Wahrscheinlichkeiten:\n",
    "\n",
    "$ P(c = 0) = \\frac{3}{5} $\n",
    "\n",
    "$ P(c = 1) = \\frac{2}{5} $\n",
    "\n",
    "Likelihoods:\n",
    "\n",
    "$ P(x_1 = 0 \\mid c = 0) = \\frac{1}{3}  $\n",
    "\n",
    "$ P(x_1 = 1 \\mid c = 0) = \\frac{2}{3}  $\n",
    "\n",
    "$ P(x_2 = 0 \\mid c = 0) = \\frac{2}{3} $\n",
    "\n",
    "$ P(x_2 = 1 \\mid c = 0) = \\frac{1}{3} $\n",
    "\n",
    "$ P(x_1 = 0 \\mid c = 1) = \\frac{1}{2} $\n",
    "\n",
    "$ P(x_1 = 1 \\mid c = 1) = \\frac{1}{2} $\n",
    "\n",
    "$ P(x_2 = 0 \\mid c = 1) = \\frac{1}{2} $\n",
    "\n",
    "$ P(x_2 = 1 \\mid c = 1) = \\frac{1}{2} $\n",
    "\n",
    "### Aufgabe 5: Entscheidungsbäume\n",
    "\n",
    "(a1)\n",
    "\n",
    "5\n",
    "\n",
    "(a2)\n",
    "\n",
    "3\n",
    "\n",
    "(a3)\n",
    "\n",
    "12\n",
    "\n",
    "(a4)\n",
    "\n",
    "$ 2 \\cdot 2 + 2 \\cdot 2 + 2 \\cdot 2 + 3 \\cdot 1 + 3 \\cdot 1 $\n",
    "\n",
    "$ = 4 + 4 + 4 + 3 + 3 = 18 $\n",
    "\n",
    "(b1)\n",
    "\n",
    "| $ A $ | $ B $ | $ C $ | $ (A \\lor \\lnot B \\land C) $ |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 1 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "| 0 | 1 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 | 1 |\n",
    "\n",
    "(b2)\n",
    "\n",
    "$ \\iota_{misclass}(D)= 1 - \\text{max}(\\frac{|\\{(\\mathbf{x}, c_1) \\in D\\}|}{|D|}, \\frac{|\\{(\\mathbf{x}, c_2) \\in D\\}|}{|D|}) $\n",
    "\n",
    "$ = 1 - \\text{max}(\\frac{5}{8}, \\frac{3}{8})  = \\frac{3}{8} $\n",
    "\n",
    "$ \\Delta\\iota = \\iota_{misclass}(D) - (\\frac{|D_1|}{|D|} \\cdot \\iota_{misclass}(D_1) + \\frac{|D_2|}{|D|} \\cdot \\iota_{misclass}(D_2)) $\n",
    "\n",
    "Merkmal A:\n",
    "\n",
    "$ \\iota(D_{A=1}) = \\frac{1}{2} $\n",
    "\n",
    "$ \\iota(D_{A=0}) = \\frac{1}{4} $\n",
    "\n",
    "$ \\Delta\\iota_A = \\frac{3}{8} - (\\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{4}) $\n",
    "\n",
    "$ = \\frac{3}{8} - \\frac{3}{8} = 0 $\n",
    "\n",
    "Merkmal B:\n",
    "\n",
    "$ \\iota(D_{B=1}) = \\frac{1}{4} $\n",
    "\n",
    "$ \\iota(D_{B=0}) = \\frac{1}{2} $\n",
    "\n",
    "$ \\Delta\\iota_B = \\frac{3}{8} - (\\frac{1}{2} \\cdot \\frac{1}{4} + \\frac{1}{2} \\cdot \\frac{1}{2}) $\n",
    "\n",
    "$ = 0 $\n",
    "\n",
    "Merkmal C:\n",
    "\n",
    "$ \\iota(D_{C=1}) = \\frac{1}{4} $\n",
    "\n",
    "$ \\iota(D_{C=0}) = 0 $\n",
    "\n",
    "$ \\Delta\\iota_C = \\frac{3}{8} - (\\frac{1}{2} \\cdot \\frac{1}{4} + \\frac{1}{2} \\cdot 0) $ \n",
    "\n",
    "$ = \\frac{3}{8} - \\frac{1}{8}  = \\frac{2}{8} $\n",
    "\n",
    "Das Merkmal mit der maximalen Unreinheitsreduktion ist $ C $."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
