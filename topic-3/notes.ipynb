{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Models\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### Binary Classification Problems\n",
    "\n",
    "Setting:\n",
    "* $ X $ is a multiset of feature vectors from an inner product space $ \\mathbf{X}, \\mathbf{X} \\in \\mathbb{R} $\n",
    "* $ C = \\{0, 1\\} $ is a set of two classes\n",
    "* $ D = \\{(\\mathbf{x}_1, c_1), \\dots, (\\mathbf{x}_n, c_n)\\} \\subseteq X \\times C $ is a multiset of examples\n",
    "\n",
    "Learning task:\n",
    "* Fit $ D $ using a logistic function $ y() $.\n",
    "\n",
    "Examples for binary classification problems:\n",
    "* E-Mail is spam or ham?\n",
    "* Patient infected or healthy?\n",
    "* Customer creditworthy or not?\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "![img1](img/topic3img1.png)\n",
    "\n",
    "* Linear Regression: $ y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} $\n",
    "* Classification: Predict \"spam\" if $ y(\\mathbf{x}) \\geq 0 $ else \"ham\"\n",
    "\n",
    "Restrict the range of $ y(\\mathbf{x}) $ to reflect the two-class classification semantics:\n",
    "\n",
    "$ -1 \\leq y(\\mathbf{x}) \\leq 1 $ or $ 0 \\leq y(\\mathbf{x}) \\leq 1 $ \n",
    "\n",
    "### Sigmoid (Logistic) Function\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Linear Regression $ \\circ $ Sigmoid Function $ \\rightarrow $ Logistic Model Function\n",
    "\n",
    "$ \\mathbf{w}^T \\mathbf{x} \\circ \\frac{1}{1 + e^{-z}} \\rightarrow y(\\mathbf{x}) \\equiv \\sigma(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1 + e^{\\mathbf{w}^T \\mathbf{x}}} $\n",
    "\n",
    "$ y: \\mathbb{R}^{p + 1} \\rightarrow (0; 1) $\n",
    "\n",
    "This is interpreted as the estimated probability for th event $ \\boldsymbol{\\mathsf{C}} = 1 $:\n",
    "* $ y(\\mathbf{x}) = P(\\boldsymbol{\\mathsf{C}}=1 \\mid \\boldsymbol{\\mathsf{X}}=\\mathbf{x}; \\mathbf{w}) =: p(1 \\mid \\mathbf{x}; \\mathbf{w}) $ \"Probability for C=1 given x, parameterized w\"\n",
    "* * $ 1- y(\\mathbf{x}) = P(\\boldsymbol{\\mathsf{C}}=0 \\mid \\boldsymbol{\\mathsf{X}}=\\mathbf{x}; \\mathbf{w}) =: p(0 \\mid \\mathbf{x}; \\mathbf{w}) $ \"Probability for C=0 given x, parameterized w\"\n",
    "\n",
    "Example (email spam classification):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\mathbf{x} = \n",
    "\\begin{pmatrix}\n",
    "x_0 \\\\ \n",
    "x_1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\ \n",
    "|\\text{obscene words}|\n",
    "\\end{pmatrix},\n",
    "\\mathbf{x}_1 = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "5\n",
    "\\end{pmatrix}\n",
    "\\text{ and }\n",
    "y(\\mathbf{x}_1) = 0.67\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "$ \\Rightarrow $ 67% chance that this email is spam.\n",
    "\n",
    "\n",
    "Recap: **Linear Regression for classification**\n",
    "![img2](img/topic3img2.png)\n",
    "\n",
    "Recap: **Logistic Regression for classification**\n",
    "![img2](img/topic3img3.png)\n",
    "\n",
    "### The BGD Algorithm\n",
    "\n",
    "Algorithm: Batch Gradient Descent\n",
    "\n",
    "Input: \n",
    "- $ D $ (multiset of examples $ (\\mathbf{x}, c) $ with $ x \\in \\mathbb{R}^p, c \\in \\{0, 1\\} $)\n",
    "- $ \\eta $ Learning rate, small positive constant\n",
    "\n",
    "Output:\n",
    "\n",
    "$ \\mathbf{w} $ weight vector from $ \\mathbb{R}^{p + 1} $ (= hypothesis)\n",
    "\n",
    "![img4](img/topic3img4.png)\n",
    "\n",
    "\n",
    "(Repeat until convergence):\n",
    "\n",
    "`FOREACH (x, c) in D DO:`\n",
    "- [Model Function evaluation]\n",
    "- [Calculation of residual]\n",
    "- [Calculation of derivative of the loss, accumulate for D]\n",
    "`ENDDO`\n",
    "- Parameter Vector update = one gradient step down\n",
    "\n",
    "![img5](img/topic3img5.png)\n",
    "\n",
    "More complex polynomials will entail more conplex decision boundaries (see lecture notes)\n",
    "\n",
    "...\n",
    "\n",
    "## Loss Computation in Detail\n",
    "\n",
    "2nd part of ML stack: \"Optimization Objective\"\n",
    "* Objective: Minimize Loss\n",
    "* Regularization: None\n",
    "* Loss: 0/1 loss, squared loss, logistic loss, cross-entropy loss, hinge loss\n",
    "\n",
    "* The pointwise loss $ l(c, y(\\mathbf{x})) $ quantifies the error introduced by some $ \\mathbf{x} $. The loss depends on the hypothesis $ y() $ and the true class $ c $ of $ \\mathbf{x} $.\n",
    "* For $ y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} $ we define the following pointwise loss functions\n",
    "  - 0/1 loss : $ l_{0/1}(c, y(\\mathbf{x})) = I_{\\neq}(c, \\text{sign}(y(\\mathbf{x}))) $ which is zero if $ c = \\text{sign}(y(\\mathbf{x})) $ and 1 otherwise\n",
    "  - Squared loss : $ l_2(c, y(\\mathbf{x})) = (c - y(\\mathbf{x}))^2 $\n",
    "\n",
    "![img6](img/topic3img6.png)\n",
    "\n",
    "* For $ y(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $ we define the following pointwise loss functions:\n",
    "  - 0/1 loss : $ l_{0/1}(c, y(\\mathbf{x})) = I_{\\neq}(c, \\lfloor y(\\mathbf{x}) + 0.5 \\rfloor) $\n",
    "  - Logistic loss : $ l_{\\sigma}(c, y(\\mathbf{x})) = -log(y(\\mathbf{x})) $ if $ c = 1 $, $ -log(1 - y(\\mathbf{x})) $ if $ c = 0$\n",
    "\n",
    "![img7](img/topic3img7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
