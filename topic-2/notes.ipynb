{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture Notes\n",
    "\n",
    "## Rule-based Learning of simple concepts\n",
    "\n",
    "> Any hypothesis foudn to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.\n",
    "\n",
    "### Find-S Algorithm\n",
    "\n",
    "The Find-S algorithm finds the most specific hyothesis that fits all positive examples of the training set.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider the following data set about which seeds are considered poisonous:\n",
    "\n",
    "| Color | Toughness | Fungus | Appearance | Poisonous |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Green | Hard | No | Wrinkled | Yes |\n",
    "| Green | Hard | Yes | Smooth | No |\n",
    "| Brown | Soft | No | Wrinkled | No |\n",
    "| Orange | Hard | No | Wrinkled | Yes |\n",
    "| Green | Hard | Yes | Wrinkled | Yes |\n",
    "| Orange | Hard | No | Wrinkled | Yes |\n",
    "\n",
    "First, we consider the hypothesis to be the most specific hypothesis:\n",
    "\n",
    "$ \\theta = (\\bot, \\bot, \\bot, \\bot) $\n",
    "\n",
    "When we consider example one, we see our initial hypothesis is more specific and we generalize its attributes.\n",
    "\n",
    "$ \\theta = (Green, Hard, No, Wrinkled) $\n",
    "\n",
    "Looking at examples two and three, we see it is not a positive example within our target class $ c $, so we ignore it and our hypothesis remains unchanged.\n",
    "\n",
    "Example four is positive, so we have to consider it. We compare each attribute of the current example with the current hypothesis and if any mismatch is found, we replace it with a general case \"$ ? $\".\n",
    "\n",
    "$ \\theta = (?, Hard, No, Wrinkled) $\n",
    "\n",
    "If we keep repeating this step for every example in the training set, we get our output hypothesis vector:\n",
    "\n",
    "$ \\theta = (?, Hard, ?, Wrinkled) $\n",
    "\n",
    "### Candidate Elimination Algorithm\n",
    "\n",
    "The CEA is a supervised technique for learning concepts from data.\n",
    "\n",
    "In concept learning, we have a dateset of objects labelled either as positive or negative, where positives are members of the target concept and negatives are not. The goal is to formulate a proper concept function of this data using the CEA.\n",
    "\n",
    "\n",
    "**Example** \n",
    "\n",
    "| Sky | AirTemp | Humidity | Wind | Water | Forecast | *EnjoySport*\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Sunny | Warm | Normal | Strong | Warm | Same | Yes |\n",
    "| Sunny | Warm | High | Strong | Warm | Same | Yes |\n",
    "| Rainy | Cold | High | Strong | Warm | Change | No |\n",
    "| Sunny | Warm | High | Strong | Cool | Change | Yes |\n",
    "\n",
    "We want a boolean function that holds true for all examples in which *EnjoySport* = Yes.\n",
    "We call the hypothesis space $ H $ the set of all candidate hypotheses that the chosen representation can express.\n",
    "\n",
    "There may be multiple hypotheses that fully capture positive elements in our data, but we want hypotheses which are consistent with the negative ones.\n",
    "\n",
    "All the functions consistent with positive and negative objects (meaning they classify them correctly) constitute the version space of the target concept.\n",
    "\n",
    "**Partial Ordering**\n",
    "\n",
    "The ordering the CEA uses is induced by the relation \"more general than or equally general as\" ($ \\geq $).\n",
    "\n",
    "The most general hypothesis is the one that labels all examples as positive. Similarly, the most specific one is the one that always returns $ false $. \n",
    "\n",
    "**Finding the Version Space**\n",
    "\n",
    "CEA processes the labelled objects one by one. If necessary, it specializes $ G $ and generalizes $ S $ so that $ V(G, S) $ (the version space whose general and specific boundaries are $ G $ and $ S $) correctly classifies all the objects processed so far.\n",
    "\n",
    "```\n",
    "Data: D (a dataset of objects labelled as positive or negative)\n",
    "Result: V (version space of hypotheses consistent with D)\n",
    "Initialize G to the set containing most general hypothesis\n",
    "Initialize S to the set containing most specific hypothesis\n",
    "\n",
    "for each object x in D:\n",
    "    if x is a positive object:\n",
    "        Remove from G any hypothesis inconsistent with x\n",
    "\n",
    "        for each hypothesis in S that is consistent with x:\n",
    "            Remove s from S\n",
    "\n",
    "            Add to S all minimal generalizations h of s such that h is consistent with x\n",
    "            and for some member g of G it holds that g >= h\n",
    "\n",
    "            Remove from S any hypothesis that's more general than another hypothesis in S\n",
    "    \n",
    "    else:\n",
    "        Remove from G any hypothesis inconsistent with x\n",
    "\n",
    "        for each hypothesis g in G that is inconsistent with x:\n",
    "            Remove g from G\n",
    "\n",
    "            Add to G all the minimal specializations h of g such that h is consistent with x\n",
    "            and for some member s of S it holds that s >= h\n",
    "\n",
    "            Remove from S any hypothesis that's more general than another hypothesis in S\n",
    "    \n",
    "Return V as V(G, S)\n",
    "```\n",
    "\n",
    "## Example\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "CEA initializes $ G $ to $ G_0 = \\{\\langle ?, ?, ?, ?, ?, ?\\rangle\\} $ and $ S $ to $ S_0 = \\{ \\langle \\bot, \\bot, \\bot, \\bot, \\bot, \\bot \\rangle\\} $.\n",
    "The space bounded by $ S_0 $ and $ G_0, V(G_0, S_0) $ is the whole hypothesis space $ H $.\n",
    "\n",
    "**Processing the first positive object**\n",
    "\n",
    "The only hypothesis in $ G $ is $ \\langle ?, ?, ?, ?, ?, ? \\rangle $. \n",
    "This is consistent with the first object $ \\langle Sunny, Warm, Normal, Strong, Warm, Same \\rangle $, so $ G_1 = G_0 $ stays the same.\n",
    "\n",
    "However, $ \\langle \\bot, \\bot, \\bot, \\bot, \\bot, \\bot \\rangle $ as the only hypothesis in $ S $ is not\n",
    "consistent with the object, so we remove it.\n",
    "The minimal generalization of $ \\langle \\bot, \\bot, \\bot, \\bot, \\bot, \\bot \\rangle $ that still covers the object $ \\langle Sunny, Warm, Normal, Strong, Warm, Same \\rangle $ is precisely the hypothesis $ \\langle Sunny, Warm, Normal, Strong, Warm, Same \\rangle $. \n",
    "It is also less general than the hypothesis in $ G_1 $, so the new specific boundary is now\n",
    "$ S_1 = \\{\\langle Sunny, Warm, Normal, Strong, Warm, Same \\rangle\\}. $\n",
    "\n",
    "**Processing the second positive object**\n",
    "\n",
    "Again, the hypothesis in $ G $ is consistent with the positive example $ \\langle Sunny, Warm, High, Strong, Warm, Same \\rangle $, so no changes to the general boundary are necessary. $ G_2 = G_1 $\n",
    "\n",
    "The hypothesis $ \\langle Sunny, Warm, Normal, Strong, Warm, Same \\rangle $ from $ S_1 $ is overly\n",
    "specific, so we remove it from the boundary. \n",
    "Its minimal generalization that covers the second positive object is $ \\langle Sunny, Warm, ?, Strong, Warm, Same \\rangle $ which is less general than $ \\langle \\bot, \\bot, \\bot, \\bot, \\bot, \\bot \\rangle $ from $ G_2 $, so $ S_2 = \\{\\langle Sunny, Warm, ?, Strong, Warm, Same \\rangle\\} $.\n",
    "\n",
    "**Processing the first negative object**\n",
    "\n",
    "The third example is negative, so we enter the `else`-block within the main loop.\n",
    "\n",
    "Since $ S_2 $ is consistent with the example $ \\langle Rainy, Cold, High, Strong, Warm, Change \\rangle $, we don't change it such that $ S_3 = S_2 $.\n",
    "\n",
    "We can see that $ \\langle ?, ?, ?, ?, ?, ? \\rangle $ is too general and remove it from the boundary.\n",
    "Next, we can see that there are six minimal specializations of the removed hypothesis that are consistent with the example:\n",
    "$ \\langle Sunny, ?, ?, ?, ?, ? \\rangle, \\langle ?, Warm, ?, ?, ?, ? \\rangle, \\langle ?, ?, Normal, ?, ?, ? \\rangle, \\langle ?, ?, ?, Weak, ?, ? \\rangle, \\langle, ?, ?, ?, ?, Cold, ? \\rangle, \\langle ?, ?, ?, ?, ?, Same \\rangle $\n",
    "\n",
    "> To specialize a hypothesis so that it correctly classifies a negative object, we replace each ? in the hypothesis with values other than the one in the object.\n",
    "So, we have $ Sunny $ instead of $ Rainy $, $ Warm $ instead of $ Cold $ and so on.\n",
    "\n",
    "However, only $ \\langle Sunny, ?, ?, ?, ?, ? \\rangle $, $ \\langle ?, Warm, ?, ?, ?, ? \\rangle $ and $ \\langle ?, ?, ?, ?, ?, Same \\rangle $ are more general than the hypothesis $ \\langle Sunny, Warm, ?, Strong, Warm, Same \\rangle $ we have in $ S_3 $.\n",
    "\n",
    "A hypothesis that isn't more general than *at least one* hypothesis in the specific boundary doesn't classify all the positive examples as positive. That's why we keep only the minimal specializations that are $ \\geq $ than the hypothesis in $ S_3 $.\n",
    "\n",
    "**Processing the third positive object**\n",
    "\n",
    "Since the hypothesis $ \\langle ?, ?, ?, ?, ?, Same \\rangle $ is inconsistent with the object $ \\langle Sunny, Warm, High, Strong, Cool, Change \\rangle $, we remove it.\n",
    "\n",
    "The object $ \\langle Sunny, Warm, High, Strong, Cool, Change \\rangle $ shows that $ S_3 $ is overly specific, so we generalize the hypothesis to $ \\langle Sunny, Warm, ?, Strong, ?, ? $ and we get a new boundary $ S_4 $.\n",
    "\n",
    "Since all examples have been processed, the algorithm returns the vector space $ V(G_4, S_4) $.\n",
    "\n",
    "### Inductive Bias\n",
    "\n",
    "> The policy by which  a [learning] algorithm generalizes from observed training examples to classify unseen instances is its inductive bias.\n",
    "> Inductive bias is the set of assumptions that, together with the training data, deductively justify the classification by the learner to future instances.\n",
    "\n",
    "* A learning algorithm without inductive bias has no directive to classifcy unseen examples. Put another way, it cannot *generalize*\n",
    "* A learning algorithm without inductive bias can only *memorize*\n",
    "\n",
    "## Regression vs Classification\n",
    "\n",
    "The most significant difference between regression and classification is that regression allows for prediction of a continuous quality, classification predicts discrete class labels.\n",
    "\n",
    "For example, regression might predict what tomorrow's temperatur is, whereas classification might predict whether tomorrow will be hot or cold.\n",
    "\n",
    "### The Linear Regression Model\n",
    "\n",
    "* Given $ \\mathbf{x} $, predict a real-valued output under a linear function:\n",
    "\n",
    "$ y(\\mathbf{x}) = w_0 + \\sum_{j = 1}^{p} w_j \\cdot x_j $\n",
    "\n",
    "* Vector notation with $ x_0 = 1 $ and $ \\mathbf{w} = (w_0, w_1, ..., w_p)^T $:\n",
    "\n",
    "$ y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} $\n",
    "\n",
    "* Given $ \\mathbf{x}_1, ..., \\mathbf{x}_n $, assess goodness of fit as residual sum of squares:\n",
    "  \n",
    "$ \\text{RSS}(\\mathbf{w}) = \\sum_{i = 1}^{n} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 $\n",
    "\n",
    "* Estimate optimum $ \\mathbf{w} $ by minimizing residual sum of squares\n",
    "\n",
    "The RSS is a measure of the discrepancy between the data and an estimation model, such as linear regression.\n",
    "\n",
    "...\n",
    "\n",
    "## Evaluating Effectiveness\n",
    "\n",
    "Let $ O $ be a finite set of objects, $ \\mathbf{X} $ the feature space associated with the model formation function $ \\alpha O \\rightarrow \\mathbf{X}, C $ a set of classes, $ y: \\mathbf{X} \\rightarrow C $ a classifier and $ \\gamma: O \\rightarrow C $ the ideal classifier to be approximated by $ y $.\n",
    "\n",
    "Let $ X = \\{\\mathbf{x} \\mid \\mathbf{x} = \\alpha(o), o \\in O\\} $ be a multiset of feature vectors and $ c_{\\mathbf{x}} = \\gamma(o), o \\in O $.\n",
    "\n",
    "The true misclassification rate of $ y $, denoted $ \\text{Err}^*(y()) $, is defined as follows:\n",
    "\n",
    "$ \\text{Err}^*(y()) = \\frac{|\\{\\mathbf{x} \\in X: y(\\mathbf{x}) \\neq c_{\\mathbf{x}}\\}|}{|X|} $\n",
    "\n",
    "Problem: Usually the total function $ \\gamma() $ and hence $ \\text{Err}^*(y()) $ is unknown.\n",
    "\n",
    "\n",
    "Problabilistic foundation of the True Misclassification Rate:\n",
    "\n",
    "Let $ \\Omega $ be a sample space, which corresponds to a set $ O $ of real-world objects and $ P $ a probability measure on $ \\mathcal{P}(\\Omega) $. Moreover, let $ \\mathbf{X} $ be a feature space with a finite number of elements, $ C $ a set of classes and $ y: \\mathbf{X} \\rightarrow C $ a classifier.\n",
    "\n",
    "We consider two types of random variables $ \\boldsymbol{\\mathsf{X}}: \\Omega \\rightarrow \\mathbf{X} $ and $ \\boldsymbol{\\mathsf{C}}: \\Omega \\rightarrow C $\n",
    "\n",
    "Then $ p(\\mathbf{x}, c), p(\\mathbf{x}, c) := P(\\boldsymbol{\\mathsf{X}=\\mathbf{x}}, \\boldsymbol{\\mathsf{C}=c}) $ is the probability of the joint event (1) to get the vector $ \\mathbf{x} \\in \\mathbf{X} $ and (2), that the respective object belongs to the class $ c \\in C $.\n",
    "\n",
    "With this, the true misclassification rate of $ y() $ can be expressed as follows:\n",
    "\n",
    "$ \\text{Err}^*(y()) = \\sum\\limits_{\\mathbf{x} \\in \\mathbf{X}} \\sum\\limits_{c \\in C} p(\\mathbf{x}, c) \\cdot I_{\\neq}(y(\\mathbf{x}), c) $\n",
    "\n",
    "where $ I_{\\neq}(y(\\mathbf{x}), c) = 0 \\text{ if } y(\\mathbf{x}) = c $, $ 1 $ otherwise.\n",
    "\n",
    "The **Bayes Classifier** returns for $ \\mathbf{x} $ the class with the highest (posterior) probability.\n",
    "\n",
    "$ y^*(\\mathbf{x}) = \\text{argmax}_{c \\in C} p(\\mathbf{x}, c) $\n",
    "\n",
    "$  \\Rightarrow $ Bayes Error:\n",
    "\n",
    "$ \\text{Err}^*(y()) = \\sum\\limits_{\\mathbf{x} \\in \\mathbf{X}} \\sum\\limits_{c \\in C} p(\\mathbf{x}, c) \\cdot I_{\\neq}(y^*(\\mathbf{x}), c) $\n",
    "\n",
    "...\n",
    "\n",
    "### Training Error\n",
    "\n",
    "Get the training error of $ y() $:\n",
    "\n",
    "$ \\text{Err}(y(), D_{tr}) = \\frac{|\\{(\\mathbf{x}, c) \\in D_{tr}: y(\\mathbf{x}) \\neq c\\}|}{|D_{tr}|} $\n",
    "\n",
    "= missclassification rate of $ y() $ on the training set.\n",
    "\n",
    "### Holdout Error\n",
    "\n",
    "Evaluation setting:\n",
    "* $ D_{test} \\subset D $ is the test set\n",
    "* $ y() $ is the classifier trained on $ D $\n",
    "* $ y'() $ is the classifier trained on $ D_{tr} = D \\setminus D_{test} $ \n",
    "\n",
    "Holdout error of $ y'(), y() $:\n",
    "\n",
    "$ \\text{Err}(y'(), D_{test}) = \\frac{|\\{(\\mathbf{x}, c) \\in D_{test}: y(\\mathbf{x}) \\neq c\\}|}{|D_{test}|} $\n",
    "\n",
    "$ \\text{Err}(y(), D_{test}) := \\text{Err}(y'(), D_{test}) $\n",
    "\n",
    "### Cross-Validation Error\n",
    "\n",
    "**$ k $-Fold-Cross-Validation**\n",
    "\n",
    "Evaluation setting:\n",
    "* $ k $ test sets $ D_{test_i} $ by splitting $ D $ into $ k $ disjoint sets of similar size\n",
    "* $ y() $ is the classfier trained on $ D $\n",
    "* $ y'(), i = 1, \\dots, k, $ are the classifiers trained on $ D_{tr} = D \\setminus D_{test_i} $ \n",
    "\n",
    "$ \\text{Err}(y(), D, k) = \\frac{1}{k} \\sum\\limits_{i = 1}^k \\frac{|\\{(\\mathbf{x}, c) \\in D_{test_i}: y_i'(\\mathbf{x}) \\neq c\\}|}{|D_{test_i}|} $\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
