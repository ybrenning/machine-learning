{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Perceptron Learning\n",
    "\n",
    "### The Perceptron of Rosenblatt (1958)\n",
    "\n",
    "![img1](img/topic6img1.png)\n",
    "\n",
    "- The weights $ w_j $ model the reinforcement factor\n",
    "- The threshold function models a decision rule\n",
    "- The perceptron is a \"feed forward system\"\n",
    "\n",
    "Rewriting the threshold as shown above and making it a constant input with a variable weight, we would end up with something like the following:\n",
    "\n",
    "![img2](img/topic6img2.png)\n",
    "\n",
    "By extending both the weight vector by $ w_0 = -\\theta $ and the feature vectors by the constant $ x_0 = 1 $, the learning algorithm gets a canonical form.\n",
    "\n",
    "### Binary Classification Problems\n",
    "\n",
    "Setting: We have $ X, C, D $. Fit $ D $ using a perceptron $ y() $.\n",
    "\n",
    "**The PT Algorithm**\n",
    "\n",
    "Algorithm: Perceptron Training\n",
    "\n",
    "Input: $ D $ with $ \\mathbf{x} \\in \\mathbb{R}^p, c \\in \\{0, 1\\} $ and $ \\mu $, a small positive constant\n",
    "\n",
    "Output: Weight vector from $ \\mathbf{w} \\in \\mathbb{R}^{p + 1} $ (hypothesis)\n",
    "\n",
    "![img3](img/topic6img3.png)\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "```\n",
    "t = t + 1\n",
    "(x, c) = random_select(D)\n",
    "Model function evaluation\n",
    "Calculation of indicator for true/false hyperplane side\n",
    "Calculation of weight correction\n",
    "Parameter vector update\n",
    "```\n",
    "\n",
    "![img4](img/topic6img4.png)\n",
    "\n",
    "Definition of an (affine) hyperplane $ N = \\{\\mathbf{x} \\mid \\vec{\\mathbf{n}}^T \\mathbf{x} = d\\} $\n",
    "- $ \\vec{\\mathbf{n}} $ is a normal vector of the hyperplane $ N $\n",
    "- if $ ||\\vec{\\mathbf{n}}|| = 1 $ then $ \\vec{\\mathbf{n}}^T \\mathbf{x} = d $ gives the (geometric) distance of a point $ \\mathbf{x} $ to $ N $\n",
    "- if $ sign(\\vec{\\mathbf{n}}^T \\mathbf{x}_1 = d) = sign(\\vec{\\mathbf{n}}^T \\mathbf{x}_2 = d) $, then $ \\mathbf{x}_1, \\mathbf{x}_2 $ are on the same side of the hyperplane\n",
    "\n",
    "![img5](img/topic6img5.png)\n",
    "\n",
    "### Example\n",
    "\n",
    "![img6](img/topic6img6.png)\n",
    "\n",
    "* The examples are presented to the perceptron\n",
    "* It computes a value that is intepreted as a calss label\n",
    "\n",
    "Encoding:\n",
    "\n",
    "* The encoding of the examples is based on features such as the number of line crossings, most acute angle, longest line etc.\n",
    "* The class label $ c $ is encoded as a number: examples from $ A $ are encoded with $ 1 $, $ B $ as $ 0 $.\n",
    "\n",
    "![img7](img/topic6img7.png)\n",
    "\n",
    "See lecture notes for step-by-step visualization of the PT-Algorithm\n",
    "\n",
    "### Perceptron Convergence Theorem\n",
    "\n",
    "Let $ X_0, X_1 $ be two finite sets with vectors of the form $ \\mathbf{x} = (1, x_1, \\dots, x_p)^T $, let $ X_1 \\cap X_0 = \\emptyset $ and let $ \\hat{\\mathbf{w}} $ define a separating hyperplane w.r.t. $ X_0, X_1 $. Moreover, let $ D $ be a set of examples of the form $ (\\mathbf{x}, 0), \\mathbf{x} \\in X_0 $ and $ (\\mathbf{x}, 1), \\mathbf{x} \\in X_1 $.\n",
    "\n",
    "Then holds:\n",
    "\n",
    "If the examples in $ D $ are processed with the PT-algorithm, the constructed weight vector $ \\mathbf{w} $ will converge withing a finite number of iterations.\n",
    "\n",
    "See lecture notes for proof.\n",
    "\n",
    "Given some $ \\mathbf{w} $, the PT algorithm checks if the examples $ (\\mathbf{x}, c) \\in D $ are on the correct hyperplane side and possibly adapts $ \\mathbf{w} $ (left). Goal is to find a *separating hyperplane* $ \\mathbf{w} $.\n",
    "\n",
    "![img8](img/topic6img8.png)\n",
    "\n",
    "If the classes are linearly separable (left), the PT algorithm will converge. If no such hyperplane exists, convergence cannot be guaranteed (right).\n",
    "\n",
    "### PT Algorithm vs Regression\n",
    "\n",
    "Given some $ \\mathbf{w} $, regression methods will calculate a loss, quantifying the \"grade of misclassification\", by exploiting both the hyperplane side and the distance, given the examples in $ D $. Goal is to find a min-loss hyperplane $ \\mathbf{w} $.\n",
    "\n",
    "![img9](img/topic6img9.png)\n",
    "\n",
    "...\n",
    "\n",
    "## Multilayer Perceptron\n",
    "\n",
    "### Linear Separability\n",
    "\n",
    "Two sets of feature vectors $ X_0, X_1 $ from a  $ p $-dimensional feature space $ \\mathbf{X} $ are called linearly separable if $ p + 1 $ real numbers $ w_0, \\dots, w_p $ exist such that the following conditions hold:\n",
    "\n",
    "1. $ \\forall \\mathbf{x} \\in X_0: \\sum\\limits_{j = 0}^{p} w_j x_j < 0 $\n",
    "2. $ \\forall \\mathbf{x} \\in X_1 \\sum\\limits_{j = 0}^{p} w_j x_j \\geq 0 $\n",
    "\n",
    "![img10](img/topic6img10.png)\n",
    "\n",
    "The XOR function defines two sets in the $ \\mathbb{R}^2 $ that are not linearly separable:\n",
    "\n",
    "![img11](img/topic6img11.png)\n",
    "\n",
    "- Specification of several hyperplanes\n",
    "- Layered combination of several perceptrons: the mutlilayer perceptron\n",
    "\n",
    "### Overcoming the Linear Separability Restriction\n",
    "\n",
    "![img12](img/topic6img12.png)\n",
    "\n",
    "For step-by-step overview, see lecture notes and lecture video.\n",
    "\n",
    "### Overcoming the Non-Differenatiablity Restriction\n",
    "\n",
    "The sigmoid function $ \\sigma() $ as threshold function.\n",
    "\n",
    "$ \\displaystyle \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\frac{d\\sigma(z)}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z)) $\n",
    "\n",
    "A perceptron with a non-linear *and* differentiable threshold function:\n",
    "\n",
    "![img13](img/topic6img13.png)\n",
    "\n",
    "Computation of the perceptron output with the sigmoid function:\n",
    "\n",
    "$ \\displaystyle y(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{w}^T \\mathbf{x}}} $\n",
    "\n",
    "![img14](img/topic6img14.png)\n",
    "\n",
    "![img15](img/topic6img15.png)\n",
    "\n",
    "### Unrestricted Classification Problems\n",
    "\n",
    "Task: $ X, C, D $ are given -- fit $ D $ using a mutlilayer perceptron $ \\mathbf{y}() $ with a sigmoid activation function.\n",
    "\n",
    "Illustration:\n",
    "\n",
    "![img16](img/topic6img16.png)\n",
    "\n",
    "### Multilayer Perceptron with Two Layers\n",
    "\n",
    "Multilayer Perceptron $ \\mathbf{y}(x) $ with a hidden layer and a $ k $-dimensional output layer:\n",
    "\n",
    "![img17](img/topic6img17.png)\n",
    "\n",
    "Model function evaluation (=forward propagation):\n",
    "\n",
    "$ \\displaystyle \\mathbf{y}(x) = \\boldsymbol{\\sigma}(W^o \\mathbf{y}^h(\\mathbf{x})) = \\boldsymbol{\\sigma}(W^o (\\binom{1}{\\boldsymbol{\\sigma}(W^h \\mathbf{x})})) $\n",
    "\n",
    "**Forward Propagation: Batch Mode**\n",
    "\n",
    "![img18](img/topic6img18.png)\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "![img19](img/topic6img19.png)\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
