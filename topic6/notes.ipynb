{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Perceptron Learning\n",
    "\n",
    "### The Perceptron of Rosenblatt (1958)\n",
    "\n",
    "![img1](img/topic6img1.png)\n",
    "\n",
    "- The weights $ w_j $ model the reinforcement factor\n",
    "- The threshold function models a decision rule\n",
    "- The perceptron is a \"feed forward system\"\n",
    "\n",
    "Rewriting the threshold as shown above and making it a constant input with a variable weight, we would end up with something like the following:\n",
    "\n",
    "[!img2](img/topic6img2.png)\n",
    "\n",
    "By extending both the weight vector by $ w_0 = -\\theta $ and the feature vectors by the constant $ x_0 = 1 $, the learning algorithm gets a canonical form.\n",
    "\n",
    "### Binary Classification Problems\n",
    "\n",
    "Setting: We have $ X, C, D $. Fit $ D $ using a perceptron $ y() $.\n",
    "\n",
    "**The PT Algorithm**\n",
    "\n",
    "Algorithm: Perceptron Training\n",
    "\n",
    "Input: $ D $ with $ \\mathbf{x} \\in \\mathbb{R}^p, c \\in \\{0, 1\\} $ and $ \\mu $, a small positive constant\n",
    "\n",
    "Output: Weight vector from $ \\mathbf{w} \\in \\mathbb{R}^{p + 1} $ (hypothesis)\n",
    "\n",
    "![img3](img/topic6img3.png)\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "```\n",
    "t = t + 1\n",
    "(x, c) = random_select(D)\n",
    "Model function evaluation\n",
    "Calculation of indicator for true/false hyperplane side\n",
    "Calculation of weight correction\n",
    "Parameter vector update\n",
    "```\n",
    "\n",
    "![img4](img/topic6img4.png)\n",
    "\n",
    "Definition of an (affine) hyperplane $ N = \\{\\mathbf{x} \\mid \\vec{\\mathbf{n}}^T \\mathbf{x} = d\\} $\n",
    "- $ \\vec{\\mathbf{n}} $ is a normal vector of the hyperplane $ N $\n",
    "- if $ ||\\vec{\\mathbf{n}}|| = 1 $ then $ \\vec{\\mathbf{n}}^T \\mathbf{x} = d $ gives the (geometric) distance of a point $ \\mathbf{x} $ to $ N $\n",
    "- if $ sign(\\vec{\\mathbf{n}}^T \\mathbf{x}_1 = d) = sign(\\vec{\\mathbf{n}}^T \\mathbf{x}_2 = d) $, then $ \\mathbf{x}_1, \\mathbf{x}_2 $are on the same side of the hyperplane\n",
    "\n",
    "![img5](img/topic6img5.png)\n",
    "\n",
    "### Example\n",
    "\n",
    "![img6](img/topic6img6.png)\n",
    "\n",
    "* The examples are presented to the perceptron\n",
    "* It computes a value that is intepreted as a calss label\n",
    "\n",
    "Encoding:\n",
    "\n",
    "* The encoding of the examples is based on features such as the number of line crossings, most acute angle, longest line etc.\n",
    "* The class label $ c $ is encoded as a number: examples from $ A $ are encoded with $ 1 $, $ B $ as $ 0 $.\n",
    "\n",
    "![img7](img/topic6img7.png)\n",
    "\n",
    "See lecture notes for step-by-step visualization of the PT-Algorithm\n",
    "\n",
    "### Perceptron Convergence Theorem\n",
    "\n",
    "Let $ X_0, X_1 $ be two finite sets with vectors of the form $ \\mathbf{x} = (1, x_1, \\dots, x_p)^T $, let $ X_1 \\cap X_0 = \\emptyset $ and let $ \\hat{\\mathbf{w}} $ define a separating hyperplane w.r.t. $ X_0, X_1 $. Moreover, let $ D $ be a set of examples of the form $ (\\mathbf{x}, 0), \\mathbf{x} \\in X_0 $ and $ (\\mathbf{x}, 1), \\mathbf{x} \\in X_1 $.\n",
    "\n",
    "Then holds:\n",
    "\n",
    "If the examples in $ D $ are processed with the PT-algorithm, the constructed weight vector $ \\mathbf{w} $ will converge withing a finite number of iterations.\n",
    "\n",
    "See lecture notes for proof.\n",
    "\n",
    "Given some $ \\mathbf{w} $, the PT algorithm checks if the examples $ (\\mathbf{x}, c) \\in D $ are on the correct hyperplane side and possibly adapts $ \\mathbf{w} $ (left). Goal is to find a *separating hyperplane* $ \\mathbf{w} $.\n",
    "\n",
    "![img8](img/topic6img8.png)\n",
    "\n",
    "If the classes are linearly separable (left), the PT algorithm will converge. If no such hyperplane exists, convergence cannot be guaranteed (right).\n",
    "\n",
    "### PT Algorithm vs Regression\n",
    "\n",
    "Given some $ \\mathbf{w} $, regression methods will calculate a loss, quantifying the \"grade of misclassification\", by exploiting both the hyperplane side and the distance, given the examples in $ D $. Goal is to find a min-loss hyperplane $ \\mathbf{w} $.\n",
    "\n",
    "![img9](img/topic6img9.png)\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
