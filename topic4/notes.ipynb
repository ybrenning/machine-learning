{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning\n",
    "\n",
    "## Probability Basics\n",
    "\n",
    "**Definition 1**\n",
    "A random experiment or a random trial is a procedure that, at least theoretically, can be repeated infinite times. It is characterized as follows:\n",
    "\n",
    "1. Configuration (A precisely specified system that can be reconstructed)\n",
    "2. Procedure (An instruction on how to execute the experiment, based on the config)\n",
    "3. Unpredictability of the outcome\n",
    "\n",
    "A set $ \\Omega = \\{\\omega_1, \\omega_2, \\dots, \\omega_n\\} $ is called *sample space* of a random experiment, if eac hexperiment outcome is associated with at most one element $ \\omega \\in \\Omega $. The elements in $ \\Omega $ are called *outcomes*.\n",
    "\n",
    "Let $ \\Omega $ be a finite sample space. Each subset $ A \\subseteq \\Omega $ is called an event, which occurs iff the experiment outcome $ \\omega $ is a member of $ A $. The set of all events $ \\mathcal{P}(\\Omega) $, is called the event space or $ \\sigma $-algebra.\n",
    "\n",
    "Ex:\n",
    "* Experiment: Rolling a dice\n",
    "* Sample Space: $ \\Omega = \\{1, 2, 3, 4, 5, 6\\} $\n",
    "* Some Event: $ A = \\{2, 4, 6\\} $\n",
    "\n",
    "* Experiment: Rolling two dice at the same time\n",
    "* Sample Space: $ \\Omega = \\{\\{1, 1\\}, \\{1, 2\\}, \\dots, \\{6, 6\\}\\} $\n",
    "* Some Event: $ B = \\{\\{1, 2\\}\\} $\n",
    "\n",
    "* Experiment: Rolling two dice in succession\n",
    "* Sample Space: $ \\Omega = \\{(1, 1), (1, 2), \\dots, (6, 6)\\} $\n",
    "* Some Event: $ C = \\{(1, 2), (2, 1)\\} $\n",
    "\n",
    "### How to capture the Nature of Probablity\n",
    "\n",
    "1. Classic, symmetry-based\n",
    "2. Frequentist\n",
    "3. Axiomatic\n",
    "4. Subjectivist, Bayesian, prognostic\n",
    "\n",
    "**Classical/Laplace Probability**\n",
    "\n",
    "If each elementary event $ \\{\\omega\\}, \\omega \\in \\Omega $ gets assigned the same probability, then the probability $ P(A) $ of an event $ A $ is defined as follows:\n",
    "\n",
    "$ P(A) = \\frac{|A|}{|\\Omega|} $\n",
    "\n",
    "**Frequentist**\n",
    "\n",
    "Basis is the **empirical law of large numbers**:\n",
    "\n",
    "In a random experiment, the average of the outcomes obtained from a large number of trials is close to the expected value, it will come closer as more trials are performed.\n",
    "\n",
    "**Axiomatic**\n",
    "\n",
    "(a) Postulate a function $ P() $ (That assigns a probability to every event in $ \\mathcal{P}(\\Omega) $)\n",
    "\n",
    "(b) Specify the required properties (of $ P() $ in the form of axioms)\n",
    "\n",
    "**Subjectivist, Bayesian, Prognostic**\n",
    "\n",
    "Consider (prior) knowledge about the hypotheses:\n",
    "\n",
    "$ p(h \\mid D) = \\frac{p(D \\mid h) \\cdot p(h)}{p(D)} $\n",
    "* Likelihood: how well does $ h $ explain (entail, induce, invoke) the data $ D $?\n",
    "* Prior: how probable is the hypothesis $ h $ a priori (in principle)?\n",
    "\n",
    "### Axiomatic Approach to Probability\n",
    "\n",
    "**Probability Measure**\n",
    "Let $ \\Omega $ be a set, called sample space, and let $ \\mathcal{\\Omega} $ be an event space. A Function $ P, P: \\mathcal{P}(\\Omega) \\rightarrow \\mathbb{R} $, which maps each event $ A $ onto a a real number $ P(A) $ is called probability measure if it has the following properties:\n",
    "\n",
    "1. $ P(A) \\geq 0 $ (Axiom I)\n",
    "2. $ P(\\Omega) = 1 $ (Axiom II)\n",
    "3. $ A \\cap B = \\emptyset \\Rightarrow P(A \\cup B) = P(A) + P(B) $ (Axiom III)\n",
    "\n",
    "**Probability Space**\n",
    "Let $ \\Omega $ be a sample space, $ \\mathcal{P}(\\Omega) $ be an event space, and $ P: \\mathcal{P}(\\Omega) \\rightarrow \\mathbb{R} $ be a probability measure. Then the tuple $ (\\Omega, P) $ as well as the tripe $ (\\Omega, \\mathcal{P}(\\Omega), P) $ is called probability space.\n",
    "\n",
    "The Kolmogorov Axioms also imply:\n",
    "1. $ P(A) + (P(\\overline{A}) = 1 $\n",
    "2. $ P(\\emptyset) = 0 $\n",
    "3. $ A \\subseteq B \\Rightarrow P(A) \\leq P(B) $\n",
    "4. $ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $\n",
    "5. Let $ A_1, A_2, \\dots A_n $ be mutually exclusive (incompatible), then holds:\n",
    "   - $ P(A_1 \\cup A_2 \\cup \\dots \\cup A_n) = P(A_1) + P(A_2) + \\dots + P(A_n) $\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "Let $ (\\Omega, \\mathcal{P}(\\Omega), P) $ be a probability space and let $ A, B \\in \\mathcal{P}(\\Omega) $ be two events. Then the probability of the occurence of event $ A $ given that event $ B $ is known to have occurred is defined as follows:\n",
    "\n",
    "$ P(A \\mid B) = \\frac{P(A \\cup B)}{P(B)} $ if $ P(B) > 0 $\n",
    "\n",
    "This is called \"probability of A under condition B\".\n",
    "\n",
    "![img1](img/topic4img1.png)\n",
    "\n",
    "### Total Probability\n",
    "\n",
    "Let $ (\\Omega, \\mathcal{P}(\\Omega), P) $ be a probability space and let $ A_1, A_2, \\dots, A_n $ be mutually exclusive events with $ \\Omega = A_1 \\cup \\dots \\cup A_n, P(A_i) > 0, i = 1, \\dots, n $. Then for each $ B \\in \\mathcal{P}(\\Omega) $ holds:\n",
    "\n",
    "$ P(B) = \\sum\\limits_{i = 1}{k} P(A_i) \\cdot P(B \\mid A_i) $\n",
    "\n",
    "![img2](img/topic4img2.png)\n",
    "\n",
    "### Independence of Events\n",
    "\n",
    "Let $ (\\Omega, \\mathcal{P}(\\Omega), P) $ be a probability space and let $ A, B \\in \\mathcal{P}(\\Omega) $ be two events. Then $ A $ and $ B $ are called statistically independet iff the following holds true:\n",
    "\n",
    "$ P(A \\cap B) = P(A) \\cdot P(B) $ (multiplication rule)\n",
    "\n",
    "$ \\Rightarrow P(A \\mid B) = P(A \\mid \\overline{B}) $\n",
    "$ \\Leftrightarrow P(A \\mid B) = P(A) $\n",
    "\n",
    "The statistical independence of $ k $ events can also be determined by checking whether the multiplication rule holds true for all subsets of the $ k $ events.\n",
    "\n",
    "## Bayes Classifier\n",
    "\n",
    "### Generative Approach to Classification Problems\n",
    "\n",
    "Setting:\n",
    "* $ X $ is a multiset of feature vectors\n",
    "* $ C $ is a set of classes\n",
    "* $ D = \\{(\\mathbf{x}_1, c), \\dots, (\\mathbf{x}_n, c)\\} \\in X \\times C $ is a multiset of examples\n",
    "\n",
    "Learning task: Fit $ D $ using joint probabilities $ p() $ between features and classes.\n",
    "\n",
    "Let $ (\\Omega, \\mathcal{P}(\\Omega), P) $ be a probability space and let $ A_1, \\dots, A_k $ be mutually exclusive events with $ \\Omega = A_1 \\cup \\dots \\cup A_k, P(A_i) > 0, i = 1, \\dots, k $. Then for an event $ B \\in \\mathcal{P}(\\Omega) $ with $ P(B) > 0 $ holds:\n",
    "\n",
    "$ P(A_i \\mid B) = \\frac{P(A_i) \\cdot P(B \\mid A_i)}{\\sum\\limits_{i = 1}^{k} P(A_i) \\cdot P(B \\mid A_i)} $\n",
    "\n",
    "$ P (A_i) $ is called a priori probability of $ A_i $.\n",
    "\n",
    "$ P(B \\mid A_i) $ is called posterior probability of $ A_i $\n",
    "\n",
    "### Example: Reasoning about a disease\n",
    "\n",
    "1. \n",
    "   - $ A_1 $: HIV_pos with $ P(A_1) = 0.001 $ (prior knowledge about population)\n",
    "   - $ A_2 $: HIV_neg with $ P(A_2) = 1 - P(A_1) = 0.999 $\n",
    "   - $ B $: test_pos\n",
    "2. $ B \\mid A_1 $: test_pos | HIV_pos with $ P(B \\mid A_1) = 0.98 $ (result from clinical trials)\n",
    "3. $ B \\mid A_2 $: test_pos | HIV_neg with $ P(B \\mid A_2) = 0.03 $ (result from clinical trials)\n",
    "\n",
    "Using the Theory of Total Probability we can deduce:\n",
    "$ \\Rightarrow P(B) = \\sum\\limits_{i = 1}^{2} P(A_i) \\cdot P(B \\mid A_i) = 0.031 $\n",
    "\n",
    "Now, we can use the simple Bayes formula to determine the probability of a patient having HIV under the condition that they have been tested positive:\n",
    "\n",
    "$ P(HIV_pos \\mid test_pos) = P(A_1 \\mid B) = \\frac{P(A_1) \\cdot P(B \\mid A_1)}{P(B)} = \\frac{0.001 \\cdot 0.98}{0.031} = 0.032 = 3.2% $\n",
    "\n",
    "![img3](img/topic4img3.png)\n",
    "\n",
    "### Combined Condiitonal Events\n",
    "\n",
    "Let $ P(A_i \\mid B_1, \\dots, B_p $ denote the probability of the occurrence of event $ A_i $ given that the events $ B_, \\dots, B_p $ are known to have occurred.\n",
    "\n",
    "Applied to a classification problem:\n",
    "* $ A_i $ corresponds to an event of the kind $ \\boldsymbol{\\mathsf{C}}=c_i $, the $ B_j, j = 1, \\dots, p $ correspond to $ p $ events of the kind $ \\boldsymbol{\\mathsf{X}}_j = x_j $\n",
    "* Observable relation (in the prevalent setting): $ B_1, \\dots, B_p \\mid A_i $\n",
    "* Reversed relation (in a diagnosis setting): $ A_i \\mid B_1, \\dots, B_p $\n",
    "\n",
    "If sufficient data for estimating $ P(A_i) $ and $ P(B_1, \\dots B_p \\mid A_i) $ is provided, then $ P(A_i, B_1, \\dots, B_p) $ can be computed with the Theorem of Bayes:\n",
    "\n",
    "$ P(A_i \\mid B_1, \\dots, B_p) = \\frac{P(A_i) \\cdot P(B_1, \\dots, B_p \\mid A_i)}{P(B_1, \\dots, B_p)} $\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "The compilation of a database from which realiable values for the $ P(B_1, \\dots, B_p \\mid A_i) $ can be obtained is often infeasable. The way out:\n",
    "\n",
    "(a) Naive Bayes Assumption: Given condition $ A_i $, the $ B_1, \\dots, B_n $ are statistically independent.\n",
    "\n",
    "Notation:\n",
    "\n",
    "$ P(B_1, \\dots, B_j \\mid A_i) = \\prod\\limits_{j = 1}^{p} P(B_j \\mid A_i) $\n",
    "\n",
    "(b) Given a set $ \\{A_1, \\dots, A_k\\} $ of alternative events (causes or classes), the most probable event under the Naive Bayes Assumption can be computed with the Theorem of Bayes:\n",
    "\n",
    "$ \\text{argmax}_{A_i \\in \\{A_1, \\dots, A_k\\}} \\frac{P(A_i) \\cdot P(B_1, \\dots, B_p \\mid A_i)}{P(B_1, \\dots B_p)} $\n",
    "\n",
    "$ = \\text{argmax}_{A_i \\in \\{A_1, \\dots, A_k\\}} P(A_i) \\cdot \\prod\\limits_{j = 1}^{p} P(B_j \\mid A_i) = A_{NB} $\n",
    "\n",
    "We can use the Naive Bayes Assumption in conjunction with a set of $ k $ mutually exclusive events $ A_i $:\n",
    "\n",
    "$ P(B_1, \\dots, B_p) = \\sum\\limits_{i = 1}^{k} P(A_i) \\cdot \\prod\\limits_{j = 1}^{p} P(B_1, \\dots, B_j \\mid A_i) $\n",
    "\n",
    "And with the Theorem of Bayes it now follows for conditional properties:\n",
    "\n",
    "$ P(A_i \\mid B_1, \\dots, B_p) = \\frac{P(A_i) \\cdot \\prod_{j = 1}^{p} P(B_j \\mid A_i)}{\\sum_{i = 1}^{k} P(A_i) \\cdot \\prod_{j = 1}^{p} P(B_j \\mid A_i)} $\n",
    "\n",
    "### Naive Bayes: Classifier Construction Summary\n",
    "\n",
    "Let $ X $ be a multiset of feature vectors, $ C $ a set of $ k $ classes and $ D \\subseteq X \\times C $ a multiset of feature examples. Then the $ k $ classes correspond to the events $ A_1, \\dots, A_k $ and the $ p $ feature values of some $ \\mathbf{x} \\in X $ correspond to the events $ B_{1=x_1}, \\dots, B_{p=x_p} $.\n",
    "\n",
    "Construction and application of a naive Bayes Classifier:\n",
    "1. Using $ D $, estimate the $ P(A_i), A_i := \\boldsymbol{\\mathsf{C}}=c_i, i = 1, \\dots, k $\n",
    "2. Using $ D $, estimate the $ P(B_{j=x_j} \\mid A_i), B_{j=x_j} := \\boldsymbol{\\mathsf{X}}_j=x_j, j = 1, \\dots, p $\n",
    "3. Classify the feature vector $ \\mathbf{x} $ as $ A_{NB} $ iff:\n",
    "   - $ A_{NB} = \\text{argmax}_{A_i \\in \\{A_1, \\dots, A_k\\}} \\hat{P}(A_i) \\cdot \\prod\\limits_{x_j \\in \\mathbf{x}, j = 1, \\dots, p} \\hat{P}(B_{j=x_j} \\mid A_i) $\n",
    "\n",
    "In this case, $ \\hat{P}() $ denotes the relative frequency, since the actual probabilities $ P() $ are unknown.\n",
    "\n",
    "### Naive Bayes: Example\n",
    "\n",
    "Compute the class $ c $ of a feature vector $ \\mathbf{x} = (sunny, cold, high, strong) $ given the following multiset of examples $ D $:\n",
    "\n",
    "![img4](img/topic4img4.png)\n",
    "\n",
    "Let $ B_{j=x_j} $ denote the event that feature $ j $ has the value $ x_j $. Then, our feature vector $ \\mathbf{x} $ gives rise to the following four events:\n",
    "\n",
    "$ B_{j=x_1}: Outlook=sunny $\n",
    "\n",
    "$ B_{j=x_2}: Temperature=cold $\n",
    "\n",
    "$ B_{j=x_3}: Humidity=high $\n",
    "\n",
    "$ B_{j=x_4}: Wind=strong $\n",
    "\n",
    "Computation of $ A_{NB} $ for $ \\mathbf{x} $ using the above formula:\n",
    "\n",
    "$ A_{NB} = \\text{argmax}_{A_i \\in \\{EnjoySurfing=yes, EnjoySurfing=no\\}} \\hat{P}(A_i) \\cdot \\hat{P}(Outlook=sunny \\mid A_i) \\cdot \\hat{P}(Temperature=cold \\mid A_i) \\cdot \\hat{P}(Humidity=high \\mid A_i) \\cdot \\hat{P}(Wind=strong \\mid A_i) $\n",
    "\n",
    "* $ \\hat{P}(EnjoySurfing=yes) = \\frac{9}{14} = 0.64 $\n",
    "* $ \\hat{P}(EnjoySurfing=no) = \\frac{5}{14} = 0.36 $\n",
    "* $ \\hat{P}(Wind=strong \\mid EnjoySurfing=yes) = \\frac{3}{9} = 0.33 $\n",
    "* ...\n",
    "\n",
    "$ \\Rightarrow $ Ranking:\n",
    "1. $ \\hat{P}(EnjoySurfing=no) \\cdot \\prod_{x_j \\in \\mathbf{x}} P(B_{j=x_j} \\mid EnjoySurfing=no) = 0.0206 $\n",
    "2. $ \\hat{P}(EnjoySurfing=yes) \\cdot \\prod_{x_j \\in \\mathbf{x}} P(B_{j=x_j} \\mid EnjoySurfing=yes) = 0.0053 $\n",
    "\n",
    "$ \\Rightarrow $ See lecture notes for calculations of final probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
