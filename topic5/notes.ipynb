{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Decision Trees Basics\n",
    "\n",
    "### Classification Problems with Nominal Features\n",
    "\n",
    "Setting:\n",
    "* $ X $ is a multiset of feature vectors\n",
    "* $ C $ is a set of classes\n",
    "* $ D \\subseteq X \\times C $ is a multiset of examples\n",
    "\n",
    "Learning task:\n",
    "* Fit $ D $ using a decision tree $ T $.\n",
    "\n",
    "**Decision Tree for the Concept \"EnjoySurfing**\n",
    "\n",
    "![img1](img/topic5img1.png)\n",
    "\n",
    "**Splitting, Induced Splitting**\n",
    "\n",
    "Let $ X $ be a set of feature vectors and $ D $ a set of examples. A splitting of $ X $ is a decomposition into mutually exclusive subsets $ X_1, \\dots, X_m $.\n",
    "\n",
    "This induces a splitting $ D_1, \\dots, D_m $ Where $ D_l, l = 1, \\dots, m $ is defined as $ \\{(\\mathbf{x}, c) \\in D \\mid \\mathbf{x} \\in X_l\\} $.\n",
    "\n",
    "![img2](img/topic5img2.png)\n",
    "\n",
    "A splitting of $ X $ depends on the measurement scale of a feature:\n",
    "\n",
    "1. $ m $-ary splitting induced by a (nominal) feature $ A $:\n",
    "   - $ dom(A) = \\{a_1, \\dots, a_m\\}: X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_1\\} \\cup \\dots \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_m\\} $\n",
    "2. Binary splitting induced by a (nominal) feature $ A $:\n",
    "   - $ B \\subset dom(A): X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\in B\\} \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\notin B\\} $\n",
    "3. Binary splitting induced by an ordinal feature $ A $:\n",
    "   - $ v \\in dom(A): X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\succeq v\\} \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\preceq v\\} $\n",
    "\n",
    "Note:\n",
    "* $ x|_A $ denotes the projection operator, which returns that vector component (dimension) of $ x $, that is associated with the feature $ A $.\n",
    "* A splitting of $ X $ into two disjoint, non-empty subsets is called a binary splitting\n",
    "\n",
    "**Decision Tree**\n",
    "\n",
    "Let $ X $ be a set of features and $ C $ be a set of classes. A decision tree $ T $ for $ X $ and $ C $ is a finite tree with a distinguished root node. A non-leaf node $ t $ has assigned (1) a set $ X(t) \\subseteq X $, (2) a splitting of $ X(t) $ and (3) a one-to-one mapping of the subsets of the splitting to its successors.\n",
    "\n",
    "Recap: $ X(t) = X $ iff $ t $ is root node, a leaf node has assigned a class from $ C $.\n",
    "\n",
    "Classification of some $ \\mathbf{x} \\in X $ given a decision tree $ T $:\n",
    "1. Find root node $ t $ of $ T $\n",
    "2. If $ t $ is a non-leaf node, find among its successors that node $ t' $ whose subset of the splitting of $ X(t') $ contains $ \\mathbf{x} $. Repeat step 2 with $ t = t' $.\n",
    "3. If $ t $ is a leaf node, label $ \\mathbf{x} $ with the associated class\n",
    "\n",
    "The set of possible decision trees over $ D $ forms the hypothesis space $ H $.\n",
    "\n",
    "### Notation\n",
    "\n",
    "Let $ T $ be a decision tree for $ X $ and $ C $, let $ D $ be a set of examples and let $ t $ be the root node.\n",
    "* $ X(t) $ denotes the subset of $ X $ that is represented by $ t $\n",
    "* $ D(t) $ denotes the subset of the example set that is represented by $ t $, where $ D(t) = \\{(\\mathbf{x}, c) \\in D \\mid \\mathbf{x} \\in X(t)\\} $.\n",
    "\n",
    "![img3](img/topic5img3.png)\n",
    "\n",
    "### Algorithm Template: Construction\n",
    "\n",
    "Algorithm: Decision Tree Construction\n",
    "\n",
    "Input: Multiset of examples $ D $\n",
    "\n",
    "Output: Root node of decision tree $ t $\n",
    "\n",
    "![img4](img/topic5img4.png)\n",
    "\n",
    "### Algorithm Template: Classification\n",
    "\n",
    "Algorithm: Decision Tree Classification\n",
    "\n",
    "Input:\n",
    "* Feature vector $ \\mathbf{x} $\n",
    "* Root node of DT $ t $\n",
    "\n",
    "Output: $ y(\\mathbf{x}) $ (class of feature vector in the decision tree)\n",
    "\n",
    "DT-Classify($ \\mathbf{x}, t $)\n",
    "\n",
    "```\n",
    "IF isLeafNode(t)\n",
    "THEN return (label(t))\n",
    "ELSE return (DT-Classify(x, splitSuccessor(t, x))\n",
    "```\n",
    "\n",
    "### When to use Decision Trees\n",
    "\n",
    "* the objects can be described by feature-value combinations\n",
    "* the domain and range of target function are discrete\n",
    "* hyptheses can be represented in DNF\n",
    "* the training set contains noise\n",
    "\n",
    "Typical application areas:\n",
    "* medical diagnosis\n",
    "* fault detection in technical systems\n",
    "* risk analysis for credit approval\n",
    "* basic scheduling tasks such as calender management\n",
    "* classification of design flaws in software engineering\n",
    "\n",
    "### Assessment of Decision Trees\n",
    "\n",
    "1. Size\n",
    "\n",
    "Among those theories that can explain an observation, the most simple one is to be preferred (Ockham's Razor)\n",
    "\n",
    "Here: Among all decision trees of minimum classification error we choose the one of smallest size\n",
    "\n",
    "2. Classification Error\n",
    "\n",
    "Quantifies the rigor according to which a class label is assigned to $ \\mathbf{x} $ in a leaf node based on the examples in the example set.\n",
    "\n",
    "If all leaf nodes of a decision tree represent a single example in $ D $, the classification error of $ T $ w.r.t. $ D $ is zero.\n",
    "\n",
    "**Assessment of Decision Trees: Size**\n",
    "\n",
    "* Leaf node number\n",
    "  - corresponds to number of rules encoded in DT\n",
    "* Tree height\n",
    "  - corresponds to max rule length and and bounds number of premises to be evaluated to reach a class decision\n",
    "* External path length\n",
    "  - totals lengths of all paths required to reach leaf nodes from root (corresponds to the total space to store all rules encoded within the decision tree)\n",
    "* Weighted external path length\n",
    "  - external path length with each length value weighted by the number of examples in $ D $ which are classified by this path\n",
    "\n",
    "Example:\n",
    "\n",
    "![img5](img/topic5img5.png)\n",
    "\n",
    "The following trees accurately classify all examples in $ D $:\n",
    "\n",
    "![img6](img/topic5img6.png)\n",
    "\n",
    "The problem to decide for a set of examples $ D $ whether or not a decision tree exists whose external path length is bounded by $ b $ is NP-complete.\n",
    "\n",
    "The class that is assigned to $ t,  label(t) $ is defined as follows:\n",
    "\n",
    "$ label(t) = \\text{argmax}_{c \\in C} |\\{(\\mathbf{x}, c) \\in D(t)\\}| $\n",
    "\n",
    "Misclassification rate of node classifier $ t $ w.r.t. $ D(t) $:\n",
    "\n",
    "$ \\text{Err}(t, D(t)) = \\frac{|\\{(\\mathbf{x}, c) \\in D: c \\neq label(t)\\}|}{|D(t)|} $\n",
    "\n",
    "$ = 1 - \\text{max}_{c \\in C} \\frac{|(\\mathbf{x}, c) \\in D(t)|}{|D(t)|} $\n",
    "\n",
    "Misclassification rate of decision tree classifier $ T $ w.r.t. $ D $:\n",
    "\n",
    "$ \\text{Err}(T, D) = \\sum\\limits_{t \\in leaves(T)}\\frac{|D(t)|}{|D|} \\cdot \\text{Err}(t, D(t)) $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
