{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Decision Trees Basics\n",
    "\n",
    "### Classification Problems with Nominal Features\n",
    "\n",
    "Setting:\n",
    "* $ X $ is a multiset of feature vectors\n",
    "* $ C $ is a set of classes\n",
    "* $ D \\subseteq X \\times C $ is a multiset of examples\n",
    "\n",
    "Learning task:\n",
    "* Fit $ D $ using a decision tree $ T $.\n",
    "\n",
    "**Decision Tree for the Concept \"EnjoySurfing**\n",
    "\n",
    "![img1](img/topic5img1.png)\n",
    "\n",
    "**Splitting, Induced Splitting**\n",
    "\n",
    "Let $ X $ be a set of feature vectors and $ D $ a set of examples. A splitting of $ X $ is a decomposition into mutually exclusive subsets $ X_1, \\dots, X_m $.\n",
    "\n",
    "This induces a splitting $ D_1, \\dots, D_m $ Where $ D_l, l = 1, \\dots, m $ is defined as $ \\{(\\mathbf{x}, c) \\in D \\mid \\mathbf{x} \\in X_l\\} $.\n",
    "\n",
    "![img2](img/topic5img2.png)\n",
    "\n",
    "A splitting of $ X $ depends on the measurement scale of a feature:\n",
    "\n",
    "1. $ m $-ary splitting induced by a (nominal) feature $ A $:\n",
    "   - $ dom(A) = \\{a_1, \\dots, a_m\\}: X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_1\\} \\cup \\dots \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_m\\} $\n",
    "2. Binary splitting induced by a (nominal) feature $ A $:\n",
    "   - $ B \\subset dom(A): X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\in B\\} \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\notin B\\} $\n",
    "3. Binary splitting induced by an ordinal feature $ A $:\n",
    "   - $ v \\in dom(A): X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\succeq v\\} \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A \\preceq v\\} $\n",
    "\n",
    "Note:\n",
    "* $ x|_A $ denotes the projection operator, which returns that vector component (dimension) of $ x $, that is associated with the feature $ A $.\n",
    "* A splitting of $ X $ into two disjoint, non-empty subsets is called a binary splitting\n",
    "\n",
    "**Decision Tree**\n",
    "\n",
    "Let $ X $ be a set of features and $ C $ be a set of classes. A decision tree $ T $ for $ X $ and $ C $ is a finite tree with a distinguished root node. A non-leaf node $ t $ has assigned (1) a set $ X(t) \\subseteq X $, (2) a splitting of $ X(t) $ and (3) a one-to-one mapping of the subsets of the splitting to its successors.\n",
    "\n",
    "Recap: $ X(t) = X $ iff $ t $ is root node, a leaf node has assigned a class from $ C $.\n",
    "\n",
    "Classification of some $ \\mathbf{x} \\in X $ given a decision tree $ T $:\n",
    "1. Find root node $ t $ of $ T $\n",
    "2. If $ t $ is a non-leaf node, find among its successors that node $ t' $ whose subset of the splitting of $ X(t') $ contains $ \\mathbf{x} $. Repeat step 2 with $ t = t' $.\n",
    "3. If $ t $ is a leaf node, label $ \\mathbf{x} $ with the associated class\n",
    "\n",
    "The set of possible decision trees over $ D $ forms the hypothesis space $ H $.\n",
    "\n",
    "### Notation\n",
    "\n",
    "Let $ T $ be a decision tree for $ X $ and $ C $, let $ D $ be a set of examples and let $ t $ be the root node.\n",
    "* $ X(t) $ denotes the subset of $ X $ that is represented by $ t $\n",
    "* $ D(t) $ denotes the subset of the example set that is represented by $ t $, where $ D(t) = \\{(\\mathbf{x}, c) \\in D \\mid \\mathbf{x} \\in X(t)\\} $.\n",
    "\n",
    "![img3](img/topic5img3.png)\n",
    "\n",
    "### Algorithm Template: Construction\n",
    "\n",
    "Algorithm: Decision Tree Construction\n",
    "\n",
    "Input: Multiset of examples $ D $\n",
    "\n",
    "Output: Root node of decision tree $ t $\n",
    "\n",
    "![img4](img/topic5img4.png)\n",
    "\n",
    "### Algorithm Template: Classification\n",
    "\n",
    "Algorithm: Decision Tree Classification\n",
    "\n",
    "Input:\n",
    "* Feature vector $ \\mathbf{x} $\n",
    "* Root node of DT $ t $\n",
    "\n",
    "Output: $ y(\\mathbf{x}) $ (class of feature vector in the decision tree)\n",
    "\n",
    "DT-Classify($ \\mathbf{x}, t $)\n",
    "\n",
    "```\n",
    "IF isLeafNode(t)\n",
    "THEN return (label(t))\n",
    "ELSE return (DT-Classify(x, splitSuccessor(t, x))\n",
    "```\n",
    "\n",
    "### When to use Decision Trees\n",
    "\n",
    "* the objects can be described by feature-value combinations\n",
    "* the domain and range of target function are discrete\n",
    "* hyptheses can be represented in DNF\n",
    "* the training set contains noise\n",
    "\n",
    "Typical application areas:\n",
    "* medical diagnosis\n",
    "* fault detection in technical systems\n",
    "* risk analysis for credit approval\n",
    "* basic scheduling tasks such as calender management\n",
    "* classification of design flaws in software engineering\n",
    "\n",
    "### Assessment of Decision Trees\n",
    "\n",
    "1. Size\n",
    "\n",
    "Among those theories that can explain an observation, the most simple one is to be preferred (Ockham's Razor)\n",
    "\n",
    "Here: Among all decision trees of minimum classification error we choose the one of smallest size\n",
    "\n",
    "2. Classification Error\n",
    "\n",
    "Quantifies the rigor according to which a class label is assigned to $ \\mathbf{x} $ in a leaf node based on the examples in the example set.\n",
    "\n",
    "If all leaf nodes of a decision tree represent a single example in $ D $, the classification error of $ T $ w.r.t. $ D $ is zero.\n",
    "\n",
    "**Assessment of Decision Trees: Size**\n",
    "\n",
    "* Leaf node number\n",
    "  - corresponds to number of rules encoded in DT\n",
    "* Tree height\n",
    "  - corresponds to max rule length and and bounds number of premises to be evaluated to reach a class decision\n",
    "* External path length\n",
    "  - totals lengths of all paths required to reach leaf nodes from root (corresponds to the total space to store all rules encoded within the decision tree)\n",
    "* Weighted external path length\n",
    "  - external path length with each length value weighted by the number of examples in $ D $ which are classified by this path\n",
    "\n",
    "Example:\n",
    "\n",
    "![img5](img/topic5img5.png)\n",
    "\n",
    "The following trees accurately classify all examples in $ D $:\n",
    "\n",
    "![img6](img/topic5img6.png)\n",
    "\n",
    "The problem to decide for a set of examples $ D $ whether or not a decision tree exists whose external path length is bounded by $ b $ is NP-complete.\n",
    "\n",
    "The class that is assigned to $ t,  label(t) $ is defined as follows:\n",
    "\n",
    "$ label(t) = \\text{argmax}_{c \\in C} |\\{(\\mathbf{x}, c) \\in D(t)\\}| $\n",
    "\n",
    "Misclassification rate of node classifier $ t $ w.r.t. $ D(t) $:\n",
    "\n",
    "$ \\displaystyle \\text{Err}(t, D(t)) = \\frac{|\\{(\\mathbf{x}, c) \\in D: c \\neq label(t)\\}|}{|D(t)|} $\n",
    "\n",
    "$ \\displaystyle = 1 - \\text{max}_{c \\in C} \\frac{|(\\mathbf{x}, c) \\in D(t)|}{|D(t)|} $\n",
    "\n",
    "Misclassification rate of decision tree classifier $ T $ w.r.t. $ D $:\n",
    "\n",
    "$ \\text{Err}(T, D) = \\sum\\limits_{t \\in leaves(T)}\\frac{|D(t)|}{|D|} \\cdot \\text{Err}(t, D(t)) $\n",
    "\n",
    "## Impurity Functions\n",
    "\n",
    "### Splitting\n",
    "\n",
    "Let $ t $ be a leaf node of an incomplete decision tree, and let $ D(t) $ be the subset of the example set $ D $ that is represented by $ t $.\n",
    "\n",
    "Possible critera for a splitting of $ X(t) $:\n",
    "1. Size of $ D(t) $ (do not split if $ |D(t)| $ is below a threshold)\n",
    "2. Purity of $ D(t) $ (do not split if all examples are members of the same class)\n",
    "3. Impurity reduction of $ D(t) $ (do not split if impurity reduction $ \\Delta \\iota $ is below a threshold)\n",
    "\n",
    "![img7](img/topic5img7.png)\n",
    "\n",
    "**Impurity function $ \\iota $**\n",
    "\n",
    "Let $ k \\in \\mathbb{N} $. An impurity function $ \\iota: [0; 1]^k \\rightarrow \\mathbb{R} $ is a function defined on the standard $ k - 1 $-simplex, denoted $ \\Delta^{k - 1} $, for which the following properties hold:\n",
    "\n",
    "(a) $ \\iota $ becomes minimum at points $ (1, 0, \\dots, 0), (0, 1, \\dots, 0), \\dots, (0, \\dots, 0, 1) $ \n",
    "\n",
    "(b) $ \\iota $ is symmetric with regard to its arguments $ p_1, \\dots, p_k $\n",
    "\n",
    "(c) $ \\iota $ becomes maximum at point $ (1/k, \\dots, 1/k) $.\n",
    "\n",
    "**Impurity of a sample set $ \\iota(D) $**\n",
    "\n",
    "Let $ X $ be a multiset of feature vectors, $ C $ a set of classes and $ D $ a multiset of examples. Morever, let $ \\iota $ be an impurity function as defined above.\n",
    "\n",
    "The impurity of $ D $, denoted $ \\iota(D) $ is defined as follows:\n",
    "\n",
    "$ \\displaystyle \\iota(D) = \\iota(\\frac{|\\{(\\mathbf{x}, c_1) \\in D\\}|}{|D|}, \\dots, \\frac{|\\{(\\mathbf{x}, c_k) \\in D\\}|}{|D|}) $\n",
    "\n",
    "**Impurity Reduction $ \\Delta \\iota $**\n",
    "\n",
    "Let $ D_1, \\dots, D_m $ be a splitting of an example set $ D $, which is induced by a splitting of $ X $, then the resulting impurity reduction, denoted as $ \\Delta \\iota (D, \\{D_1, \\dots, D_m\\}) $, is defined as follows:\n",
    "\n",
    "$ \\displaystyle \\Delta \\iota(D, \\{D_1, \\dots, D_m\\}) = \\iota(D) - \\sum\\limits_{l=1}^{m} \\frac{|D_l|}{|D|} \\cdot \\iota(D_l) $\n",
    "\n",
    "### Impurity Functions Based on the Misclassification Rate\n",
    "\n",
    "Definition for two classes:\n",
    "\n",
    "$ \\displaystyle \\iota_{misclass}(p_1, p_2) = 1 - \\text{max}\\{p_1, p_2\\} $\n",
    "\n",
    "... Which is $ p_1 $ if $ 0 \\leq p_1 \\leq 0.5 $ or $ 1 - p_1 $ otherwise.\n",
    "\n",
    "$ \\displaystyle \\iota_{misclass}(D) = 1 - \\text{max}\\{\\frac{|(\\mathbf{x}, c_1) \\in D|}{|D|}, \\frac{|(\\mathbf{x}, c_2) \\in D|}{|D|}\\} $\n",
    "\n",
    "Definition for $ k $ classes:\n",
    "\n",
    "$ \\iota_{misclass}(p_1, \\dots, p_k) = 1 - \\text{max}_{i = 1, \\dots, k} p_i $\n",
    "\n",
    "$ \\iota_{misclass}(D) = 1 - \\text{max}_{c \\in C} \\frac{(|\\mathbf{x}, c) \\in D|}{|D|} $\n",
    "\n",
    "Problems:\n",
    "* $ \\Delta \\iota_{misclass} = 0 $ max hold for all possible splittings\n",
    "* the impurity function that is induced by the misclassification rate undererstimates pure nodes\n",
    "\n",
    "![img8](img/topic5img8.png)\n",
    "\n",
    "![img9](img/topic5img9.png)\n",
    "\n",
    "See also:\n",
    "* Strict impurity Function\n",
    "\n",
    "### Impurity Functions based on Entropy\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "Let $ A $ denote an event and let $ P(A) $ denote the occurrence probability of $ A $. Then the entropy (self-information, information content) of $ A $ is defined as $ -\\log_2(P(A)) $.\n",
    "\n",
    "Let $ \\mathcal{A} $ be an experiment with the exclusive outcomes (events) $ A_1, \\dots A_k $. Then the mean information content of this experiment, denoted with $ H(\\mathcal{A}) $, is called Shannon entropy or entropy of experiment $ \\mathcal{A} $ and is defined as follows:\n",
    "\n",
    "$ \\displaystyle H(\\mathcal{A}) = - \\sum\\limits_{i = 1}^{k} P(A_i) \\cdot \\log_2(P(A_i)) $\n",
    "\n",
    "See also:\n",
    "* Conditional Entropy, Information Gain\n",
    "* Impurity functions based on entropy for 2, $ k $ classes \n",
    "* Impurity functions based on the Gini Index\n",
    "\n",
    "## Decision Tree Algorithms\n",
    "\n",
    "### ID3 Algorithm\n",
    "\n",
    "Setting: $ X $ is a multiset of feature vectors, $ C $ a set of classes, $ D $ a multiset of examples\n",
    "\n",
    "Learning task:\n",
    "\n",
    "Fit $ D $ using a decision tree $ T $.\n",
    "\n",
    "Characteristics of the ID3 Algorithm:\n",
    "1. Each splitting is based on one nominal feature and considers its complete domain. Splitting based on feature $ A $ with domain $ dom(A) = \\{a_1, \\dots, a_nm\\}: X = \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_1\\} \\cup \\dots \\cup \\{\\mathbf{x} \\in X: \\mathbf{x}|_A = a_m\\} $\n",
    "2. Splitting  criterion is *information gain*\n",
    "\n",
    "**ID3(D, Features)**\n",
    "1. Create a node t for the tree\n",
    "2. Label t with the most common class in D\n",
    "3. If all examples in D have the same class, return single-node tree t\n",
    "4. If Features is empty, return single-node tree t\n",
    "   - Otherwise:\n",
    "5. Let A* be the feature from Features that best classifies examples in D. Assign t the desicion feature A*.\n",
    "6. For each possible value \"a\" in dom(A*) do:\n",
    "   - Add a new tree branch below t, corresponding to the test A*=\"a\"\n",
    "   - Let D_a be the subset of D that has a value \"a\" for A.\n",
    "   - If D_a is empty:\n",
    "    * Then add a leaf node with the label of the most common class in D\n",
    "    * Else add a subtree ID3(D_A, Features \\ A*)\n",
    "7. Return t\n",
    "\n",
    "![img10](img/topic5img10.png)\n",
    "\n",
    "### Example of ID3\n",
    "\n",
    "![img11](img/topic5img11.png)\n",
    "\n",
    "![img12](img/topic5img12.png)\n",
    "\n",
    "For visualization of decision tree between recursive calls, see lecture notes.\n",
    "\n",
    "Observations:\n",
    "* Decision tree search happens in the space of all hypotheses\n",
    "* To generate a DT, the ID3 algorithm needs per branch at most as many decisions as features are given\n",
    "\n",
    "Where the inductive bias of ID3 becomes manifest:\n",
    "1. Small decision trees are preferred\n",
    "2. Highly discriminative features tend to be closer to the root\n",
    "\n",
    "### CART Algorithm\n",
    "\n",
    "The same setting as ID3, this time no restrictions are presumed for the features measurement scales in $ X $.\n",
    "\n",
    "Task: Fit $ D $ using a decision tree $ T $.\n",
    "\n",
    "Characteristics of CART:\n",
    "* Eacch splitting is binary and considers one feat at a time\n",
    "* Splitting criterion is the information gain or the Gini Index\n",
    "\n",
    "> See lecture notes for CART steps\n",
    "\n",
    "## Decision Tree Pruning\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "![img14](img/topic5img14.png)\n",
    "\n",
    "Recall overfitting from section *Overfitting* in part Linear Models. The hypothesis $ h \\in H $ is considered to overfit $ D $ if the hypothesis $ h' \\in H $ with the following property exists:\n",
    "* $ Acc(h, D) > Acc(h', D) $ and $ Acc^*(h) < Acc^*(h') $\n",
    "\n",
    "Note that $ Acc $ is the percentage of correctly classified examples, i.e. $ 1 - Err $\n",
    "\n",
    "**Lemma 10**\n",
    "Let $ t $ be a node of a decision tree $ T $. Then, for each induced splitting $ D(t_1), \\dots, D(t_m) $ of a set of examples $ D(t) $ then holds:\n",
    "\n",
    "$ \\displaystyle Err(t, D(t)) > \\sum\\limits_{i \\in \\{1,\\dots,m\\}} Err(t_i, D(t_i)) $\n",
    "\n",
    "Approaches to counter overfitting:\n",
    "\n",
    "(a) Stopping of the DT construction process during training\n",
    "\n",
    "(b) Pruning of a DT after training\n",
    "* Splitting of DT into three sets for training, validation and test\n",
    "  - reduced error pruning\n",
    "  - min cost complexity pruning\n",
    "  - rule post pruning\n",
    "* statistical tests such as $ \\chi^2 $ to assess generalization capability\n",
    "* heuristic pruning\n",
    "\n",
    "### (a) Stopping\n",
    "\n",
    "Possible criteria for stopping:\n",
    "1. Size of D(t) (do not split if |D(t)| is below certain threshold)\n",
    "2. Purity of D(t) (do not split if all examples in D(t) are in same class)\n",
    "3. Impurity reduction of D(t) (do not split if impurity reduction $ \\Delta\\iota $ is below threshold)\n",
    "\n",
    "Problems:\n",
    "1. A threshold that is too small results in oversized DTs\n",
    "   - A threshold too large omits useful splittings\n",
    "2. Perfect purity cannot be expected with noisy data\n",
    "3. $ \\Delta\\iota $ cannot be extrapolated with regard to the tree height\n",
    "\n",
    "### (b) Pruning\n",
    "\n",
    "The principle:\n",
    "1. Construct a sufficiently large DT $ T_{max} $\n",
    "2. Prune $ T_{max} $, starting from leaf nodes upwards to root\n",
    "\n",
    "Each leaf node of $ T_{max} $ fulfills one or more of the following conditions:\n",
    "- $ D(t) $ is sufficiently small, Typically $ |D(t)| < 5 $\n",
    "- $ D(t) $ is pure\n",
    "- $ D(t) $ is comprised of examples with identical feature vectors\n",
    "\n",
    "**Decision Tree Pruning**\n",
    "\n",
    "Given a DT $ T $ and an inner (non-root and non-leaf) node $ t $. Then the pruning of $ T $ with regard to $ t $ is the removal of all successor nodes of $ t $ in $ T $. The pruned tree is denoted as $ T \\setminus T_t $. The node $ t $ now becomes a leaf node of $ T \\setminus T_t $.\n",
    "\n",
    "![img13](img/topic5img13.png)\n",
    "\n",
    "**Pruning-Induced Ordering**\n",
    "\n",
    "Let $ T' $ and $ T $ be two DTs. Then $ T' \\preceq  T $ denotes the fact that $ T' $ is the result of a (possibly repeated) pruning applied to $ T $. The relation $ \\preceq $ forms a partial ordering on the set of all trees.\n",
    "\n",
    "Problems when assessing pruning candidates:\n",
    "- Pruned DTs may not stand in the $ \\preceq $-relation\n",
    "- Locally optimum pruning decisions may not result in the best candidates\n",
    "- Its monotonicity disqualifies $ Err_{tr}(T) $ as an estimator for $ Err^*(T) $\n",
    "\n",
    "Control pruning with a validation set $ D_{val} $:\n",
    "1. $ D_{test} \\subset D $, test set for DT assessment after pruning\n",
    "2. $ D_{val} \\subset (D \\setminus D_{test}) $, validation set for overfitting analysis during pruning\n",
    "3. $ D_{tr} = D \\setminus (D_{test} \\cup D_{val}) $, training set for DT construction\n",
    "\n",
    "### Reduced Error Pruning\n",
    "\n",
    "Steps of reduced error pruning:\n",
    "1. $ T = T_{max} $\n",
    "2. Choose inner node $ t $ in $ T $\n",
    "3. Perform a tentative pruning of $ T $ with regard to $ t: T' = T \\setminus T_t $\n",
    "4. If $ Err(T', D_{val}) \\leq Err(T, D_{val}) $ then accept the pruning: $ T = T' $\n",
    "5. Continue with step 2 until all inner nodes of $ T $ are nested\n",
    "\n",
    "Problem:\n",
    "\n",
    "If $ D $ is small, its partitioning into three sets for training, validation and testing will discard valuable information for DT construction.\n",
    "\n",
    "Improvement: Rule Post Pruning\n",
    "\n",
    "![img15](img/topic5img15.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0620f40e2353b6d35268ab276de4393f4e99801270e1c452fb1fe73380c5b64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
